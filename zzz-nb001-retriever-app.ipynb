{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d733e1a-fff5-4739-b702-cc4af0a69d41",
   "metadata": {},
   "source": [
    "# How to build a simple Retriever LLM App with LangChain\n",
    "* Very simple Retriever LLM App over a text data source. \n",
    "* Retriever Apps can answer questions about specific documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48386f20-c929-48a9-8720-0953fcd67dd0",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65ee060-21f2-4e01-b283-1fd656dac1e9",
   "metadata": {},
   "source": [
    "#### After you download the code from the github repository in your computer\n",
    "In terminal:\n",
    "* cd project_name\n",
    "* pyenv local 3.11.4\n",
    "* poetry install\n",
    "* poetry shell\n",
    "\n",
    "#### To open the notebook with Jupyter Notebooks\n",
    "In terminal:\n",
    "* jupyter lab\n",
    "\n",
    "Go to the folder of notebooks and open the right notebook.\n",
    "\n",
    "#### To see the code in Virtual Studio Code or your editor of choice.\n",
    "* open Virtual Studio Code or your editor of choice.\n",
    "* open the project-folder\n",
    "* open the 001-retriever-app.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08dda73-8cde-4772-8612-7cb9f1ccc2c6",
   "metadata": {},
   "source": [
    "## Create your .env file\n",
    "* In the github repo we have included a file named .env.example\n",
    "* Rename that file to .env file and here is where you will add your confidential api keys. Remember to include:\n",
    "* OPENAI_API_KEY=your_openai_api_key\n",
    "* LANGCHAIN_TRACING_V2=true\n",
    "* LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
    "* LANGCHAIN_API_KEY=your_langchain_api_key\n",
    "* LANGCHAIN_PROJECT=your_project_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a63ff6-99ff-4629-b965-547d12a99ba6",
   "metadata": {},
   "source": [
    "We will call our LangSmith project **000-retriever-app**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8d60b0-9de8-4711-a5b9-74aeb9d04422",
   "metadata": {},
   "source": [
    "## Connect with the .env file located in the same directory of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddec1158-3cef-4292-af21-939b97c09a95",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d991590d-c63d-43c1-bd27-3f688510791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "992ec4a9-aa01-4e44-aeb9-b9a1f3aa9e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c01d18b-f9f0-427b-a9dc-3d1885160578",
   "metadata": {},
   "source": [
    "#### Install LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b0fe96-f4ae-4344-9984-3f5398947034",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed746499-d1b8-41e5-b131-270cf5fa229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2af3ef-c2c7-445f-92bd-a29c68abce25",
   "metadata": {},
   "source": [
    "## Connect with an LLM and start a conversation with it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cea739e-0f01-4503-a6f9-ab00b14dc8b1",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fa7337f-3d60-4ede-bdf8-aa7a5cffec43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce3551e-95ca-41a1-8810-89c495bf93ab",
   "metadata": {},
   "source": [
    "* For this project, we will use OpenAI's gpt-3.5-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9afcbc7d-a816-41e3-925f-850883f5770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9913fc8c-254f-410d-aa8f-35eb0898855e",
   "metadata": {},
   "source": [
    "#### Track the operation in LangSmith\n",
    "* [Open LangSmith here](smith.langchain.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ee94ed-0021-49a3-ba1b-977f6a4440c9",
   "metadata": {},
   "source": [
    "## Install Chroma Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c723b5-65ae-4af4-813f-5ed0514e451e",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "735752cc-ecb5-4942-9ed0-b3213f5768c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langchain-chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c061f1-54c7-4061-9cba-002643ab0b8a",
   "metadata": {},
   "source": [
    "## Documents\n",
    "* A LangChain Document is intended to store text and metadata.\n",
    "* Have 2 attributes:\n",
    "    * `page_content`\n",
    "    *  `metadata`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d522841b-b079-48d4-9458-9fe3bf13aa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"John F. Kennedy served as the 35th president of the United States from 1961 until his assassination in 1963.\",\n",
    "        metadata={\"source\": \"us-presidents-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Robert F. Kennedy was a key political figure and served as the U.S. Attorney General; he was also assassinated in 1968.\",\n",
    "        metadata={\"source\": \"us-politics-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"The Kennedy family is known for their significant influence in American politics and their extensive philanthropic efforts.\",\n",
    "        metadata={\"source\": \"kennedy-family-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Edward M. Kennedy, often known as Ted Kennedy, was a U.S. Senator who played a major role in American legislation over several decades.\",\n",
    "        metadata={\"source\": \"us-senators-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Jacqueline Kennedy Onassis, wife of John F. Kennedy, was an iconic First Lady known for her style, poise, and dedication to cultural and historical preservation.\",\n",
    "        metadata={\"source\": \"first-lady-doc\"},\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00291675-15c1-46bf-af88-1b5b4d82c00a",
   "metadata": {},
   "source": [
    "## Vector Stores vs. Retrievers\n",
    "Let's break down the differences between **vector stores** and **retrievers** in a way that's easy to understand.\n",
    "\n",
    "#### Vector Stores\n",
    "Think of a vector store as a specialized storage space where information is kept in a very specific format:\n",
    "- **Storing Vectors**: A vector store keeps information as vectors. These vectors are numerical representations of text, making it easier for machines to understand and compare information quickly.\n",
    "- **Purpose**: The main goal of a vector store is to efficiently store and retrieve these vectors. When you need to find how similar two pieces of information are, the vector store helps by quickly comparing their vectors.\n",
    "- **Usage**: They are crucial in systems where you need to perform similarity searches over large datasets. For example, finding documents that discuss similar topics or identifying similar user queries.\n",
    "\n",
    "#### Retrievers\n",
    "On the other hand, retrievers are more about actively finding information:\n",
    "- **Retrieving Information**: A retriever takes a query (like a question or a search term) and looks through a database to find relevant information.\n",
    "- **Purpose**: The purpose of a retriever is to sift through large amounts of data and bring back the most relevant documents or entries that answer the query.\n",
    "- **Usage**: Retrievers are used in search engines, question-answering systems, and anywhere you need to pull out specific pieces of information from a large dataset quickly.\n",
    "\n",
    "### Key Differences\n",
    "- **Functionality**: Vector stores are focused on storing and retrieving numerical data representations, making them ideal for tasks that involve measuring similarity. Retrievers, meanwhile, are geared towards searching through text or data to find relevant information based on a query.\n",
    "- **Output**: Vector stores return vectors or scores based on similarity measures, whereas retrievers provide a list of documents or data entries that are deemed relevant to the query.\n",
    "- **Role in Systems**: Vector stores often serve as a backend component that supports the function of retrievers by providing the necessary data representations for comparison. Retrievers use this data to perform their role of finding and fetching relevant information.\n",
    "\n",
    "In summary, vector stores and retrievers both help manage and utilize large data sets, but they do so in different ways. Vector stores focus on the storage and retrieval of data in a numerical format, while retrievers focus on retrieving relevant textual or data entries based on specific queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579929a0-a846-4b52-8e25-8b3a55dd507a",
   "metadata": {},
   "source": [
    "## Vector stores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39044438-8fcd-423e-aec6-15ff76f29753",
   "metadata": {},
   "source": [
    "We can use many vector stores in our LangChain applications. Here we will use a Chorma vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04b5a8d6-a090-42f8-bc41-d22304cc8bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents,\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a15d97f-9e31-40aa-bd0a-5ef863815b07",
   "metadata": {},
   "source": [
    "#### similarity_search()\n",
    "Imagine you have a large box full of various toys, and you're looking for toys that are similar to your favorite toy car. You might start by pulling out toys that are also cars, but then you narrow it down further to find cars that are the same color or size as your favorite one.\n",
    "\n",
    "In computer terms, similarity search works similarly. It involves searching through a vast amount of data (like all those toys) to find items that are similar to a specific item you're interested in. This could be text, images, or any type of data.\n",
    "\n",
    "When you use similarity search in a LangChain app here's how it typically goes:\n",
    "1. **Representation**: converting words or sentences into numerical forms (called embeddings).\n",
    "2. **Comparison**: Once everything is converted into numbers, compare these numbers to see how similar they are. This is like measuring the distance between two points.\n",
    "3. **Retrieval**: The retriever then sorts these items by how similar they are to your query (what you're searching for) and shows you the results that are most similar.\n",
    "\n",
    "The similarity_search() function returns documents based on similarity to a string query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f1f0889-d00e-4832-855d-2fb9c0193984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'us-presidents-doc'}, page_content='John F. Kennedy served as the 35th president of the United States from 1961 until his assassination in 1963.'),\n",
       " Document(metadata={'source': 'us-senators-doc'}, page_content='Edward M. Kennedy, often known as Ted Kennedy, was a U.S. Senator who played a major role in American legislation over several decades.'),\n",
       " Document(metadata={'source': 'us-politics-doc'}, page_content='Robert F. Kennedy was a key political figure and served as the U.S. Attorney General; he was also assassinated in 1968.'),\n",
       " Document(metadata={'source': 'first-lady-doc'}, page_content='Jacqueline Kennedy Onassis, wife of John F. Kennedy, was an iconic First Lady known for her style, poise, and dedication to cultural and historical preservation.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search(\"John\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bf492e-90fd-4834-bf6e-b32df5527a11",
   "metadata": {},
   "source": [
    "#### similarity_search_with_score()\n",
    "When we talk about `similarity_search_with_score` we're looking at a slightly more detailed process than just finding similar items. Here's how you can understand it:\n",
    "\n",
    "1. **Input and Representation**: \n",
    "    - First, you have a query, which is what you're interested in finding similar items for. This could be a piece of text, like a question or a topic.\n",
    "    - The system converts this query and all potential items that could be similar (like documents or pieces of text) into a numerical form that represents their meanings. This is usually done using models that produce embeddings.\n",
    "\n",
    "2. **Scoring Similarities**:\n",
    "    - Once everything is converted into these numerical embeddings, the system calculates the 'distance' between your query's embedding and the embeddings of other items. Closer distances mean they are more similar.\n",
    "    - The system uses a similarity score to quantify how close or far each item is from your query. This score is typically between 0 and 1, where 1 means extremely similar and 0 means not similar at all.\n",
    "\n",
    "3. **Ranking and Retrieval**:\n",
    "    - Based on these scores, the system ranks all the items from most similar to least similar.\n",
    "    - It then presents you with a list of items, each with a similarity score showing how closely it matches your query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f307c3f-4452-40e9-80ce-90e957512eb2",
   "metadata": {},
   "source": [
    "When using similarity search tools like the ones discussed in the LangChain course, these tools often convert text into numerical forms, or vectors, to measure how similar they are to each other. However, the way these vectors are stored and compared can differ depending on the tool or provider you use—each one might have its own method for scoring the similarities.\n",
    "\n",
    "Unlike some other tools that give a similarity score where a higher number means more similar, Chroma does the opposite. It uses a distance metric for scoring. In this case:\n",
    "- **A smaller distance means more similarity**: If the distance score is close to 0, it suggests that the items are very similar.\n",
    "- **A larger distance means less similarity**: If the distance score is higher, it suggests that the items are quite different.\n",
    "\n",
    "So, in simple terms, when you’re using Chroma's vector store for similarity search, remember that you’re looking for smaller numbers (or distances) to find more similar items, as these scores vary inversely with the similarity—smaller is better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91ada400-5357-4ec7-a5ad-493d6959c0e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'source': 'us-presidents-doc'}, page_content='John F. Kennedy served as the 35th president of the United States from 1961 until his assassination in 1963.'),\n",
       "  0.4497259855270386),\n",
       " (Document(metadata={'source': 'us-senators-doc'}, page_content='Edward M. Kennedy, often known as Ted Kennedy, was a U.S. Senator who played a major role in American legislation over several decades.'),\n",
       "  0.4639798402786255),\n",
       " (Document(metadata={'source': 'us-politics-doc'}, page_content='Robert F. Kennedy was a key political figure and served as the U.S. Attorney General; he was also assassinated in 1968.'),\n",
       "  0.47490057349205017),\n",
       " (Document(metadata={'source': 'first-lady-doc'}, page_content='Jacqueline Kennedy Onassis, wife of John F. Kennedy, was an iconic First Lady known for her style, poise, and dedication to cultural and historical preservation.'),\n",
       "  0.48075956106185913)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search_with_score(\"John\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b77e46-0901-49ba-85fd-75f05161bb28",
   "metadata": {},
   "source": [
    "## Retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da56a12-8fa3-4e64-a3ee-ad9ea498426a",
   "metadata": {},
   "source": [
    "* We can create a retriever manually, but this is not the option we will use most frequently. Once we choose what method we wish to use to retrieve documents, we can create a retriever using RunnableLambda. The code below will build a retriever around the similarity_search method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2660f3f1-bf66-4a2c-8f36-d957f58133e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'us-presidents-doc'}, page_content='John F. Kennedy served as the 35th president of the United States from 1961 until his assassination in 1963.')],\n",
       " [Document(metadata={'source': 'us-politics-doc'}, page_content='Robert F. Kennedy was a key political figure and served as the U.S. Attorney General; he was also assassinated in 1968.')]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "retriever = RunnableLambda(vectorstore.similarity_search).bind(k=1)  # select top result\n",
    "\n",
    "retriever.batch([\"John\", \"Robert\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ee1c57-76d9-4191-9293-ee62c37d3aa9",
   "metadata": {},
   "source": [
    "* Most of the times we will use the .as_retriever() function to create a Retriever using the vector store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f9cb56d-4cca-4aba-acdd-914396cf5837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'us-presidents-doc'}, page_content='John F. Kennedy served as the 35th president of the United States from 1961 until his assassination in 1963.')],\n",
       " [Document(metadata={'source': 'us-politics-doc'}, page_content='Robert F. Kennedy was a key political figure and served as the U.S. Attorney General; he was also assassinated in 1968.')]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 1},\n",
    ")\n",
    "\n",
    "retriever.batch([\"John\", \"Robert\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0a60c5-ecda-4a19-9d91-0a21719535b6",
   "metadata": {},
   "source": [
    "#### Retrievers are runnables\n",
    "LangChain VectorStore objects are not Runnables, and so they cannot immediately be integrated into LangChain Expression Language chains. On the contrary, LangChain Retrievers are Runnables.\n",
    "* See how we use a retriever inside a LCEL chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "569041b3-76ac-4e29-8aa9-ac03ec4888ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "message = \"\"\"\n",
    "Answer this question using the provided context only.\n",
    "\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"human\", message)])\n",
    "\n",
    "chain = {\n",
    "    \"context\": retriever, \n",
    "    \"question\": RunnablePassthrough()} | prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e82211d4-fe0d-4fae-a0e3-0b4ade757b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jackie was the wife of John F. Kennedy and an iconic First Lady known for her style, poise, and dedication to cultural and historical preservation.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(\"tell me about Jackie\")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e371e8a3-f100-4aa6-a26e-41985713998d",
   "metadata": {},
   "source": [
    "## How to execute the code from Visual Studio Code\n",
    "* In Visual Studio Code, see the file 004-invoke-stream-batch.py\n",
    "* In terminal, make sure you are in the directory of the file and run:\n",
    "    * python 001-retriever-app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32525994-0b48-4928-a277-508a24f77926",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
